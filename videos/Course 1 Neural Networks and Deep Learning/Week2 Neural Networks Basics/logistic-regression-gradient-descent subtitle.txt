welcome back in this video we'll talk about how to compute derivatives for you to infinite gradient descent for logistic regression the key takeaways will be what you need to implement that are the key equations you need in order to implement gradient descent for logistic regression but in this video I want to do this computation using the computation graph I have to admit using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression but I want to start explaining things this way to get you familiar with these ideas so that hopefully you'll make a bit more sense when we talk about full fledged neural networks but so that does dive into gradient descent for logistic regression to recap we had set up logistic regression as follows your predictions y hat is defined as follows where Z is that and if we focus on just one example for now then the loss or respect to that one example is defined as follows where a is the output of the just regression and Y is the ground truth label so let's write this out as a computation graph and for this example let's say we have only two features x1 and x2 so in order to compute Z we'll need to input w1 w2 and B in addition to the feature values x1 x2 so these things in a computation graph get used to compute Z which is w1 x1 plus w2 x2 plus B draw or tend to the box around that and then we compute Y hat or a equals Sigma of Z that's the next step in a computation Draft and then finally we compute L a Y and I won't copy the formula again so in logistic regression what we want to do is to modify the parameters W and B in order to reduce this loss we've described before propagation steps of how you actually compute the loss on a single training example now let's talk about how you can go backwards to talk to compute the derivatives here's the cleaned up version of the diagram because what we want to do is compute derivatives respect to this loss the first thing we want to do we're going backwards is to compute the derivative of this loss with respect to the script over there with respect to this variable a and so in the code you know you just use da right to denote this um variable and it turns out that if you often believe of calculus you can show that this ends up being negative Y over a plus one minus y over one minus a and the way you do that is you take the formula for the loss and if you have a bit of calculus you can compute the derivative with respect to the variable lowercase a and you get this formula but if you're not familiar of calculus don't worry about it we'll provide the derivative formulas you need throughout this course so if you are next to in calculus you'll encourage you to look up the formula for the loss from their previous slide and try to get director for respect to a using you know calculus but if you don't know enough calculus to do that don't worry about it now having computed this quantity or da the derivative of your final output variable respect to a you can then go backwards and it turns out that you can show DZ which this is the Python code variable name this is going to be you know the derivative of the loss versus back to Z or for L you can really write the loss including a and Y explicitly as parameters or not right give either type of notation is equally acceptable they can show that this is equal to a minus y um just a couple comments only for those of you did are explained experts in calculus if you're not explain calculus don't worry about it but it turns out that this right DL DZ this can be expressed as DL da time da DZ um and it turns out that da DZ this turns out to be a times 1 minus a and D l da we are previously worked out over here and so if you take these two quantities you know DL da which is this term together with da DZ which is this term and just take these two things and multiply them you can show that you the equation simplifies the a minus y so that's how you derive it and this is really the chain rule that I pre-cleared you did to inform ok so feel free to go through that calculation yourself if you are knowledgeable calculus but if you aren't all you need to know is that you can compute DZ as a minus y and it already done the calculus for you and then the final step in back propagation is to go back to compute how much you need to change W and B so in particular you can show that the derivative respect to w1 and in Co will call this DW 1 that this is equal to x1 times DZ um and then similarly DW 2 which is how much you want to change W 2 is X 2 times DZ and be excuse me DB is equal to DZ so if you want to do gradient descents with respect to just this one example what you will do is the following you would use this formula to compute DZ and then use these formulas to compute DW 1 DW 2 and DB and then you perform these updates w1 gets updated w1 - learning rate alpha times D w1 w2 gets updated similarly and B gets set as B - the learning rate times DB and so this will be one step of grade with respect to a single example so you've seen how to compute derivatives and implement gradient descent for logistic regression with respect to a single training example but to train divisions of Russian model you have not just one training example given entire training set of M training examples so in the next video let's see how you can take these ideas and apply them to learning not just from one example but from an entire training set