{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving DNN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Iterative Process\n",
    "<img src=\"images/iterative-process.png\" width=400 />\n",
    "\n",
    "## 2- Parameters and Hyperparameters\n",
    "- L: Number of Layers\n",
    "- layers_dims[L]: Number of Hiddlen Units (neurons) per layer\n",
    "- learning_rate: Learning Rate $\\alpha$ \n",
    "- Activation Functions: sigmoid, tanh, relu\n",
    "- $\\lambda$ regularization parameter\n",
    "\n",
    "## 3 - Applications\n",
    "- NLP\n",
    "- Vision\n",
    "- Speech\n",
    "- Structual Data\n",
    "    - Security\n",
    "    - Advertisment\n",
    "    - Search\n",
    "    - Logistics\n",
    "    \n",
    "## 4 - Data set\n",
    "- Training Set \n",
    "- Development Set, aka Hold out cross validation set \n",
    "- Test Set\n",
    "> Make sure training and test set from same distribution\n",
    "\n",
    "### 4.1 -  Split of Dataset\n",
    "#### Small/Medium Dataset \n",
    "100-100K samples, 60/20/20 or 70/30/.\n",
    "\n",
    "#### Big Dataset\n",
    "more than 1M samples\n",
    "dev and test set take much smaller percentage\n",
    "98/1/1, or even 99.5/.25/.25, 99.5/.4/.1\n",
    "\n",
    "\n",
    "## 5 - Workflow\n",
    "\n",
    "1. Keep on training different models on training set;\n",
    "2. Use development set to see which model performs best on dev set;\n",
    "3. After it's done long enough, take that best model and validate on test set, in order to get unbiased estimate result to make sure how the algorithm you found is doing.\n",
    "\n",
    "## 6- Bias and Variance\n",
    "<img src=\"images/bias-variance.png\" width=600 />\n",
    "> High training error => High Bias = Underfitting\n",
    "\n",
    "> High dev error => High Varaince = Overfitting\n",
    "\n",
    "| Error | Case 1 | Case 2 | Case 3 | Case 4 |\n",
    "|------:|-------:|-------:|-------:|-------:|\n",
    "| Train Set Error|1%|15%|15%|0.1%|\n",
    "| Dev Set Error|11%|16%|30%|1%|\n",
    "|**Diagnosis**|High Variance|High Bias|High Bias,High Variance|Low Bias,Low Variance|\n",
    "\n",
    "> Optimal Error is called **Bayes Error**\n",
    "\n",
    "* Reduce Bias/Underfitting\n",
    "    * Bigger Network\n",
    "    * Traning Longer\n",
    "\n",
    "* Reduce Variance/Overfitting\n",
    "    * More Data\n",
    "    * Regularization\n",
    "\n",
    "## 7 - Basic Receipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg contentScriptType=\"application/ecmascript\" contentStyleType=\"text/css\" height=\"593px\" preserveAspectRatio=\"none\" style=\"width:457px;height:593px;\" version=\"1.1\" viewBox=\"0 0 457 593\" width=\"457px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" zoomAndPan=\"magnify\"><defs><filter height=\"300%\" id=\"fta69tex2abbv\" width=\"300%\" x=\"-1\" y=\"-1\"><feGaussianBlur result=\"blurOut\" stdDeviation=\"2.0\"/><feColorMatrix in=\"blurOut\" result=\"blurOut2\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0\"/><feOffset dx=\"4.0\" dy=\"4.0\" in=\"blurOut2\" result=\"blurOut3\"/><feBlend in=\"SourceGraphic\" in2=\"blurOut3\" mode=\"normal\"/></filter></defs><g><polygon fill=\"#FBFB77\" filter=\"url(#fta69tex2abbv)\" points=\"20,10,20,65.9316,435,65.9316,435,20,425,10,20,10\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"425\" x2=\"425\" y1=\"10\" y2=\"20\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"435\" x2=\"425\" y1=\"20\" y2=\"20\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"390\" x=\"26\" y=\"27.5684\">No bias varaince tradeoff any more, if we can make NN bigger</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"379\" x=\"26\" y=\"42.8789\">and at same time train more data with proper regularization,</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"219\" x=\"26\" y=\"58.1895\">we can decrease bias and varaince.</text><rect fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" height=\"34.1328\" rx=\"12.5\" ry=\"12.5\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"107\" x=\"174\" y=\"176.0664\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"87\" x=\"184\" y=\"197.668\">bigger network</text><rect fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" height=\"34.1328\" rx=\"12.5\" ry=\"12.5\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"86\" x=\"184.5\" y=\"239.2217\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"66\" x=\"194.5\" y=\"260.8232\">train longer</text><polygon fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" points=\"200.5,119.9316,254.5,119.9316,266.5,131.9316,254.5,143.9316,200.5,143.9316,188.5,131.9316,200.5,119.9316\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"19\" x=\"231.5\" y=\"154.5664\">Yes</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"54\" x=\"200.5\" y=\"136.0889\">high bias?</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"15\" x=\"173.5\" y=\"129.6113\">No</text><rect fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" height=\"34.1328\" rx=\"12.5\" ry=\"12.5\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"80\" x=\"187.5\" y=\"378.9893\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"60\" x=\"197.5\" y=\"400.5908\">More Data</text><rect fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" height=\"34.1328\" rx=\"12.5\" ry=\"12.5\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"102\" x=\"176.5\" y=\"442.1445\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"82\" x=\"186.5\" y=\"463.7461\">Regularization</text><polygon fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" points=\"189,322.8545,266,322.8545,278,334.8545,266,346.8545,189,346.8545,177,334.8545,189,322.8545\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"19\" x=\"231.5\" y=\"357.4893\">Yes</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"77\" x=\"189\" y=\"339.0117\">high variance?</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"15\" x=\"162\" y=\"332.5342\">No</text><polygon fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" points=\"227.5,75.9316,239.5,87.9316,227.5,99.9316,215.5,87.9316,227.5,75.9316\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><polygon fill=\"#FEFECE\" filter=\"url(#fta69tex2abbv)\" points=\"200.5,518.2773,254.5,518.2773,266.5,530.2773,254.5,542.2773,200.5,542.2773,188.5,530.2773,200.5,518.2773\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"54\" x=\"200.5\" y=\"534.4346\">high bias?</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"18\" x=\"266.5\" y=\"527.957\">yes</text><ellipse cx=\"227.5\" cy=\"572.2773\" fill=\"none\" filter=\"url(#fta69tex2abbv)\" rx=\"10\" ry=\"10\" style=\"stroke: #000000; stroke-width: 1.0;\"/><ellipse cx=\"228\" cy=\"572.7773\" fill=\"#000000\" filter=\"url(#fta69tex2abbv)\" rx=\"6\" ry=\"6\" style=\"stroke: none; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"210.1992\" y2=\"239.2217\"/><polygon fill=\"#A80036\" points=\"223.5,229.2217,227.5,239.2217,231.5,229.2217,227.5,233.2217\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"143.9316\" y2=\"176.0664\"/><polygon fill=\"#A80036\" points=\"223.5,166.0664,227.5,176.0664,231.5,166.0664,227.5,170.0664\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"273.3545\" y2=\"283.3545\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"293\" y1=\"283.3545\" y2=\"283.3545\"/><polygon fill=\"#A80036\" points=\"289,219.2217,293,209.2217,297,219.2217,293,215.2217\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"293\" x2=\"293\" y1=\"131.9316\" y2=\"283.3545\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"293\" x2=\"266.5\" y1=\"131.9316\" y2=\"131.9316\"/><polygon fill=\"#A80036\" points=\"276.5,127.9316,266.5,131.9316,276.5,135.9316,272.5,131.9316\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"188.5\" x2=\"162\" y1=\"131.9316\" y2=\"131.9316\"/><polygon fill=\"#A80036\" points=\"158,205.2217,162,215.2217,166,205.2217,162,209.2217\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"162\" x2=\"162\" y1=\"131.9316\" y2=\"295.3545\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"162\" x2=\"227.5\" y1=\"295.3545\" y2=\"295.3545\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"295.3545\" y2=\"322.8545\"/><polygon fill=\"#A80036\" points=\"223.5,312.8545,227.5,322.8545,231.5,312.8545,227.5,316.8545\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"413.1221\" y2=\"442.1445\"/><polygon fill=\"#A80036\" points=\"223.5,432.1445,227.5,442.1445,231.5,432.1445,227.5,436.1445\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"346.8545\" y2=\"378.9893\"/><polygon fill=\"#A80036\" points=\"223.5,368.9893,227.5,378.9893,231.5,368.9893,227.5,372.9893\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"476.2773\" y2=\"486.2773\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"290.5\" y1=\"486.2773\" y2=\"486.2773\"/><polygon fill=\"#A80036\" points=\"286.5,422.1445,290.5,412.1445,294.5,422.1445,290.5,418.1445\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"290.5\" x2=\"290.5\" y1=\"334.8545\" y2=\"486.2773\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"290.5\" x2=\"278\" y1=\"334.8545\" y2=\"334.8545\"/><polygon fill=\"#A80036\" points=\"288,330.8545,278,334.8545,288,338.8545,284,334.8545\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"177\" x2=\"164.5\" y1=\"334.8545\" y2=\"334.8545\"/><polygon fill=\"#A80036\" points=\"160.5,408.1445,164.5,418.1445,168.5,408.1445,164.5,412.1445\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"164.5\" x2=\"164.5\" y1=\"334.8545\" y2=\"498.2773\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"164.5\" x2=\"227.5\" y1=\"498.2773\" y2=\"498.2773\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"498.2773\" y2=\"518.2773\"/><polygon fill=\"#A80036\" points=\"223.5,508.2773,227.5,518.2773,231.5,508.2773,227.5,512.2773\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"99.9316\" y2=\"119.9316\"/><polygon fill=\"#A80036\" points=\"223.5,109.9316,227.5,119.9316,231.5,109.9316,227.5,113.9316\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"266.5\" x2=\"305\" y1=\"530.2773\" y2=\"530.2773\"/><polygon fill=\"#A80036\" points=\"301,315.3545,305,305.3545,309,315.3545,305,311.3545\" style=\"stroke: #A80036; stroke-width: 1.5;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"305\" x2=\"305\" y1=\"87.9316\" y2=\"530.2773\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"305\" x2=\"239.5\" y1=\"87.9316\" y2=\"87.9316\"/><polygon fill=\"#A80036\" points=\"249.5,83.9316,239.5,87.9316,249.5,91.9316,245.5,87.9316\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"227.5\" x2=\"227.5\" y1=\"542.2773\" y2=\"562.2773\"/><polygon fill=\"#A80036\" points=\"223.5,552.2773,227.5,562.2773,231.5,552.2773,227.5,556.2773\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><!--\n",
       "@startuml\n",
       "\n",
       "floating note right\n",
       "No bias varaince tradeoff any more, if we can make NN bigger \n",
       "and at same time train more data with proper regularization, \n",
       "we can decrease bias and varaince.\n",
       "end note\n",
       "\n",
       "repeat\n",
       "while(high bias?) is (Yes)\n",
       "    :bigger network;\n",
       "    :train longer;\n",
       "endwhile (No)        \n",
       "while(high variance?) is (Yes)\n",
       "    :More Data;\n",
       "    :Regularization;\n",
       "endwhile (No)\n",
       "repeat while (high bias?) is (yes)\n",
       "stop\n",
       "\n",
       "@enduml\n",
       "\n",
       "PlantUML version 1.2017.15(Mon Jul 03 09:45:34 PDT 2017)\n",
       "(GPL source distribution)\n",
       "Java Runtime: Java(TM) SE Runtime Environment\n",
       "JVM: Java HotSpot(TM) 64-Bit Server VM\n",
       "Java Version: 1.8.0_144-b01\n",
       "Operating System: Mac OS X\n",
       "OS Version: 10.12\n",
       "Default Encoding: UTF-8\n",
       "Language: en\n",
       "Country: US\n",
       "--></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "\n",
    "floating note right\n",
    "No bias varaince tradeoff any more, if we can make NN bigger \n",
    "and at same time train more data with proper regularization, \n",
    "we can decrease bias and varaince.\n",
    "end note\n",
    "\n",
    "repeat\n",
    "while(high bias?) is (Yes)\n",
    "    :bigger network;\n",
    "    :train longer;\n",
    "endwhile (No)        \n",
    "while(high variance?) is (Yes)\n",
    "    :More Data;\n",
    "    :Regularization;\n",
    "endwhile (No)\n",
    "repeat while (high bias?) is (yes)\n",
    "stop\n",
    "\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## 1 - L2 Regularization\n",
    "\n",
    "### 1.1 - Logistic Regression\n",
    "$$min_{W,b}J(W,b), W \\in R^{nx}, b \\in R\\tag{1}$$\n",
    "$$J(W, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})\\tag{2}$$\n",
    "$$J(W, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}, y)+\\frac{\\lambda}{2m} ||w||^{2}_{2}\\tag{3}$$\n",
    "$$||w||^{2}_{2} = \\sum_{j=1}^{nx}w_{j}^{2} = w^Tw$$\n",
    "> L2 Norm $||W||^{2}_{2}$, aka Euclidian Norm\n",
    "\n",
    "> L2 Regularization\n",
    "\n",
    "> $\\lambda$ is called regularization parameter, which is used to penaltize the weight matrixes from being too large. \n",
    "\n",
    "### 1.2 - Neural Network\n",
    "$$J(w^{[1]},b^{[1]}, ... , w^{[L]},b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})\\tag{4}$$\n",
    "$$J(w^{[1]},b^{[1]}, ... , w^{[L]},b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}||w^{[l]}||^{2}_{F} \\tag{5}$$\n",
    "$$||w^{[l]}||^{2}_{F} = \\sum_{i=1}^{n^{[l]}}\\sum_{j=1}^{n^{[l-1]}}(w^{[l]}_{ij})^{2}$$\n",
    "> $w^{[l]}$ is matrix of dimension ($n^{[l]}, n^{[l-1]}$)\n",
    "\n",
    "> $||w^{[l]}||^{2}_{F}$ is called Frobenius Norm of matrix.\n",
    "\n",
    "$$dw^{[l]}=(from\\ backprop)\\tag{6}$$\n",
    "$$w^{[l]} = w^{[l]} - \\alpha dw^{[l]} \\tag{7}$$\n",
    "$$dw^{[l]}=(from\\ backprop) + \\frac{\\lambda}{m}w^{[l]}\\tag{8}$$\n",
    "$$w^{[l]} = (1-\\frac{\\alpha\\lambda}{m})w^{[l]} - \\alpha dw^{[l]} \\tag{9}$$\n",
    "\n",
    "> Formula 9 tells us why it's called \"weight decay\"\n",
    "\n",
    "#####  1.2.1 - Intuition -1\n",
    "<img src=\"images/regularization-intuition.png\" width=500/>\n",
    "> If $\\lambda$ is very large, then w will be near 0. The effect is to zero out many weights and so simplify the neural network.\n",
    "\n",
    "####  1.2.2 - Ituition -2\n",
    "<img src=\"images/regularization-intuition2.png\" width=500 />\n",
    "> With larger $\\lambda$, trained w would be smaller, and so $z = w.a + b$ would be smaller and more possible to be in (-1, 1) for tanh or (0, 1) for sigmoid. The neural net would be more linear. \n",
    "\n",
    "> With regularization, the J cost function would be more monotonically decreasing with regard to iterations. \n",
    "\n",
    "## 2 - Dropout Regularization\n",
    "<img src=\"images/dropout-regularization.png\" width=500/>\n",
    "> Dropout drop a percentage of neurons \n",
    "\n",
    "### 2.1 - Inverted Dropout Implementation\n",
    "$$\\tag{10} $d^{[l]} = np.random.rand(a^{[l]}.shape)$$\n",
    "$$\\tag{11} $a^{[l]} = np.multiply(a^{[l]}, d^{[l]})$$\n",
    "$$\\tag{12} $a^{[l]} /= keep_prob $$\n",
    "> $keep\\_prob^{[l]}$ is the layer l's keeping probability, that is 1 - drop probability.\n",
    "\n",
    "> $d^{[l]}$ is the drop out factor vector of layer l, same dimention of $a^{[l]}$\n",
    "\n",
    "### 2.2 - Don't use dropout in inference \n",
    "> Inference: make predictions at test time\n",
    "\n",
    "### 2.3 - Intuition 1\n",
    "> drop-out ramdonly drops out units in the network. On every interation smaller network has regularizing effect.\n",
    "\n",
    "### 2.4- Intuition 2\n",
    "> Can't rely on any one feature, so have to spread out wights. Spreading out weight has same effect of shrinking weights as L2 regularization, but more adaptive and scalable to input size.\n",
    "\n",
    "### 2.5 - Implementation Notes\n",
    "- Different layers may have different keep_prob\n",
    "- Computer vision use dropout a lot\n",
    "- If no overfitting then no necessarity of using dropout\n",
    "- J is not well defined any more after using dropout. Make sure J is monotonically decreasing first, then enable dropout.\n",
    "\n",
    "## 3- Data Augmentation \n",
    "<img src=\"images/data-augmentation.png\" width=500/>\n",
    "\n",
    "## 4 - Early Dropping\n",
    "<img src=\"images/early-stopping.png\" width=500 />\n",
    "\n",
    "###  4.1- Drawback\n",
    "<img src=\"images/early-stopping-drawback.png\" width=500 />\n",
    ">  The machine learning process comprise several different steps. One, is that you want an algorithm to optimize the cost function j and we have various tools to do that, such as grade descent. And then we'll talk later about other algorithms, like momentum and RMS prop and Atom and so on. But after optimizing the cost function j, you also wanted to not over-fit. And we have some tools to do that such as your regularization, getting more data and so on. Now in machine learning, we already have so many hyper-parameters it surge over. It's already very complicated to choose among the space of possible algorithms. And so I find machine learning easier to think about when you have one set of tools for optimizing the cost function J, and when you're focusing on authorizing the cost function J. All you care about is finding w and b, so that J(w,b) is as small as possible. You just don't think about anything else other than reducing this. And then it's completely separate task to not over fit, in other words, to reduce variance. And when you're doing that, you have a separate set of tools for doing it. And this principle is sometimes called **orthogonalization**. And there's this idea, that you want to be able to think about one task at a time. I'll say more about orthorganization in a later video, so if you don't fully get the concept yet, don't worry about it. But, to me the main downside of early stopping is that this couples these two tasks. So you no longer can work on these two problems independently, because by stopping gradient decent early, you're sort of breaking whatever you're doing to optimize cost function J, because now you're not doing a great job reducing the cost function J. You've sort of not done that that well. And then you also simultaneously trying to not over fit. So instead of using different tools to solve the two problems, you're using one that kind of mixes the two. And this just makes the set of things you could try are more complicated to think about. \n",
    "\n",
    "> Rather than using early stopping, one alternative is just use L2 regularization then you can just train the neural network as long as possible. I find that this makes the search space of hyper parameters easier to decompose, and easier to search over. But the downside of this though is that you might have to try a lot of values of the regularization parameter lambda. And so this makes searching over many values of lambda more computationally expensive. And the advantage of early stopping is that running the gradient descent process just once, you get to try out values of small w, mid-size w, and large w, without needing to try a lot of values of the L2 regularization hyperparameter lambda.\n",
    "\n",
    "> I personally prefer to just use L2 regularization and try different values of lambda. That's assuming you can afford the computation to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Normalization\n",
    "<img src=\"images/normalize.png\" width=500 />\n",
    "\n",
    "## Intuition\n",
    "<img src=\"images/normalize-intuition.png\" width=500 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vanishing and Exploding Gradients\n",
    "<img src=\"images/varnishing-exploding-gradients.png\" width=500 />\n",
    "\n",
    "## Xaiver and He initialization\n",
    "A well chosen initialization can:\n",
    "- Speed up the convergence of gradient descent\n",
    "- Increase the odds of gradient descent converging to a lower training (and generalization) error \n",
    "\n",
    "### Initialization\n",
    "1.Zero Initialization: \n",
    "\n",
    ">bad performance\n",
    "\n",
    "2.Random Initialization: \n",
    "\n",
    ">varnishing and exploding gradients problem\n",
    "\n",
    "3.Xavier Initialization(formula 1)\n",
    "\n",
    ">scaling factor for the weights $W^{[l]}$ of `sqrt(1./layers_dims[l-1])`\n",
    "\n",
    "4.He Initialization(formula 2)\n",
    ">This is named for the first author of He et al., 2015. \n",
    "\n",
    ">Scaling factor for the weights $W^{[l]}$ of `sqrt(2./layers_dims[l-1])`\n",
    "\n",
    "$$z^{[l]} = w^{[l]}a^{[l-1]}$$\n",
    "$$Var(w) = 1/n^{[l-1]}, for\\ tanh$$\n",
    "$$Var(w) = 2/n^{[l-1]}, for\\ relu$$\n",
    "$$\\tag{1}w^{[l]} = np.ramdom.randn(shape) * np.sqrt(1/n^{[l-1]}), for\\ tanh$$\n",
    "$$\\tag{2}w^{[l]} = np.ramdom.randn(shape) * np.sqrt(2/n^{[l-1]}), for\\ relu$$\n",
    "\n",
    "> Inutition: The larger $n^{[l-1]}$, we need w to be smaller.\n",
    "\n",
    "> Partial solution to Vanishing and Exploding Gradients problem.\n",
    "\n",
    "> Forumla 2 is called He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2. / layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Checking\n",
    "<img src=\"images/grad-check1.png\" width=500 />\n",
    "<img src=\"images/grad-check2.png\" width=500 />\n",
    "<img src=\"images/grad-check3.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
