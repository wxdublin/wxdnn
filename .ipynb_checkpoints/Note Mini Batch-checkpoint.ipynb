{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-batch gradient descent\n",
    "<img src=\"images/mini-batch-notation.png\" width=600 />\n",
    "<img src=\"images/mini-batch-alg.png\" width=600 />\n",
    "<img src=\"images/mini-batch-intuition.png\" width=600 />\n",
    "<img src=\"images/mini-batch-intuition2.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch Gradient Descent**: whole set of examples\n",
    "\n",
    "> **Mini Batch Gradient Descent**: split whole set of examples into sets of \"mini batches\"\n",
    "\n",
    "> **Stochastics Gradient Descent**: one example is one min-batch\n",
    "\n",
    "> the i_th mini batch, **$X^{\\{i\\}}$**, in dimension ($n_{x}$, mini_batch_size)\n",
    "\n",
    "> **$X^(i)$** is the i_th sample\n",
    "\n",
    "> **$Z^{[l]}$** is the layer l inputs\n",
    "\n",
    "> **epoch**: a pass of a mini batch through training set\n",
    "\n",
    "> If small data set (m<2000), use batch gradient descent\n",
    "\n",
    "> Otherwise, typical mini-batch size is 64, 128, 256, 512; at most 1024\n",
    "\n",
    "> Make sure mini-batch fit in CPU/GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
