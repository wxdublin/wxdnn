{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "> Normalize activations in a network\n",
    "\n",
    "<img src=\"images/normalization.png\" width=400 />\n",
    ">There is some debate in the deep learning literature about whether you should normalize the value before the activation function, so Z[2], or whether you should normalize the value after applying deactivation function a[2]. In practice, normalizing Z[2] is done much more often\n",
    "\n",
    "<img src=\"images/batch-normalization.png\" width=400 />\n",
    "\n",
    "> And so the way you fit this unit in your network is whereas previously you are using these values Z(1), Z(2), and so on, you would now use Ztilde(i) instead of Z(i) for the later computations in your neural network. \n",
    "\n",
    ">So the intuition I hope you take away from this is that we saw how normalizing the input features X can help learning in the neural network. And what Batch Norm does is it applies that normalization process not just to the input layer but to the values even deep in some hidden layer in the neural networks. You apply this type of normalization to normalize the mean and variance of some of your hidden units values Z. But one difference between the training input and these hidden unit values is you might not want your hidden unit values to be forced to mean zero and variance one. For example, if you have a sigmoid activation function, you don't want your values to always be clustered here, you might want them to have a larger variance or have a mean that's different than zero in order to better take advantage of the non-linearity of the sigmoid function rather than have all your values be in just this the linear version. So that's why with the parameters gamma and beta you can now make sure that your Z(i) values have the range of values that you want.\n",
    "\n",
    ">what it does really is that it ensures that your hidden units have standardized mean and variance where the mean and variance are controlled by two explicit parameters, gamma and beta, which the learning algorithm can set to whatever it wants. So what it really does is it normalizes the mean and variance of these hidden unit values, really, the Z[i]s, to have some fixed mean and variance. And that mean and variance could be zero on one or it could be some other value and it's controlled by these parameters gamma and beta. \n",
    "\n",
    "<img src=\"images/batch-normalization2.png\" width=400 />\n",
    "\n",
    "<img src=\"images/batch-normalization-minibatch.png\" width=400 />\n",
    ">But what that means is that, whatever is the value of BL is actually going to just get subtracted out, because during that Batch Normalization step, you are going to compute the means of the ZL's and subtract the mean. And so adding any constant to all of the examples in the mini-batch, it doesn't change anything. Because any constant you add will get cancelled out by the mean subtractions step. So, if you're using Batch Norm, you can actually eliminate that parameter, or if you want, think of it as setting it permanently to 0. \n",
    "\n",
    "<img src=\"images/batch-normalization-minibatch-implement.png\" width=400 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
